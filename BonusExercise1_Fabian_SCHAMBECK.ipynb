{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fubotz/MCMLR_2024W/blob/main/Bonus_Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Exercises 1: Aligning Multilingual Embedding Spaces**\n",
        "\n",
        "\n",
        "\n",
        "This notebook represents the first bonus exercises for the lecture Multilingual and Crosslingual Methods and Language Resources (2024W 340168-1). For each successfully completed bonus exercise, a maximum of three points can be achieved that will be added to the points of the final exam. The tasks to be completed in the following notebook are marked with ðŸ‘‹ âš’.\n"
      ],
      "metadata": {
        "id": "H_RsHNVC57Tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this notebook, you will perform and evaluate a supervised method for aligning the embedding spaces of two languages. The examples in the notebook rely on the language pair English-German, however, feel free to change this pair to languages of your choice from the available embeddings and dictionaries (see below)."
      ],
      "metadata": {
        "id": "UADeK6Y-6Zk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Preparing the Embeddings and Data**\n",
        "\n",
        "In this notebook, we will be using fastText embeddings that represents a character-based version of the word2vec skipgram method. Details on the method can be found in the [original publication](https://aclanthology.org/Q17-1010.pdf) and [this website](https://fasttext.cc/).\n",
        "\n",
        "Pretrained fastText embeddings are available in [157 languages](https://fasttext.cc/docs/en/crawl-vectors.html). The following code cell loads the fastText embeddings for English and German.\n",
        "\n",
        "ðŸ‘‹ âš’ Please change the following download command if you wish to align other languages than English and German.\n",
        "\n",
        "Before you decide on a final language pair, please make sure that:\n",
        "1.   There are pretrained embeddings for this language (see [here](https://fasttext.cc/docs/en/crawl-vectors.html))\n",
        "2.   There is a bilingual word list available (see the [MUSE GitHub](https://github.com/facebookresearch/MUSE/tree/main) section \"Ground-truth bilingual dictionaries\")\n",
        "\n",
        "If the embeddings are available, change the two-digit ISO code in `cc.en.300.vec.g` and `cc.de.300.vec.gz` to the language(s) of your choice."
      ],
      "metadata": {
        "id": "lmoyUnpZKmma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz        # English\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz        # German"
      ],
      "metadata": {
        "id": "DkTFQP9X-IU5",
        "collapsed": true,
        "outputId": "957d0d80-1545-47c5-a8e5-dfc8c38bc76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-04 16:14:30--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.24.51, 3.163.24.87, 3.163.24.72, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.24.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: â€˜cc.en.300.vec.gzâ€™\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G   162MB/s    in 9.0s    \n",
            "\n",
            "2025-01-04 16:14:39 (141 MB/s) - â€˜cc.en.300.vec.gzâ€™ saved [1325960915/1325960915]\n",
            "\n",
            "--2025-01-04 16:14:39--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.24.51, 3.163.24.87, 3.163.24.72, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.24.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1278030050 (1.2G) [binary/octet-stream]\n",
            "Saving to: â€˜cc.de.300.vec.gzâ€™\n",
            "\n",
            "cc.de.300.vec.gz    100%[===================>]   1.19G  33.6MB/s    in 32s     \n",
            "\n",
            "2025-01-04 16:15:11 (38.0 MB/s) - â€˜cc.de.300.vec.gzâ€™ saved [1278030050/1278030050]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Embeddings\n",
        "\n",
        "As a next step we will unzip and load the embeddings. For this alignment task, we will only use the top 100,000 words for both languages to speed up the processing. This choice of only using the top 100,000 words also depends on the lenght of the available bilingual word lists."
      ],
      "metadata": {
        "id": "r9LI8R2Q9bPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HQFF13y09xgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4a2b71-c252-47b7-8d29-a9bb8e99f45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100000 English embeddings\n",
            "Loaded 100000 German embeddings\n",
            "Top 10 English words: [',', 'the', '.', 'and', 'to', 'of', 'a', '</s>', 'in', 'is']\n",
            "Top 10 German words: [',', '.', '</s>', 'und', 'der', ':', 'die', '\"', ')', '(']\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "def load_fasttext_embeddings(file_path, top_n):\n",
        "    embeddings = {}\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            # Line 0 is a header line\n",
        "            if i > 0 and i <= top_n:\n",
        "              tokens = line.decode('utf-8').strip().split(' ')\n",
        "              word = tokens[0]\n",
        "              vector = np.array(tokens[1:], dtype=np.float32)\n",
        "              vector = vector / np.linalg.norm(vector)\n",
        "              embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Load the top English and German embeddings for the top 100,000 words (100000)\n",
        "# FastText sorts the embeddings by decreasing order of word frequency by default\n",
        "en_embeddings = load_fasttext_embeddings('cc.en.300.vec.gz', 100000)\n",
        "de_embeddings = load_fasttext_embeddings('cc.de.300.vec.gz', 100000)\n",
        "\n",
        "print(f\"Loaded {len(en_embeddings)} English embeddings\")\n",
        "print(f\"Loaded {len(de_embeddings)} German embeddings\")\n",
        "\n",
        "print(\"Top 10 English words:\", list(en_embeddings.keys())[:10])     # NB: fasttest_embeddings sorted by frequency\n",
        "print(\"Top 10 German words:\", list(de_embeddings.keys())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us explore the format of the downloaded and loaded embeddings."
      ],
      "metadata": {
        "id": "yhaTWGHKEr1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The loaded embeddings represent a {type(en_embeddings)} datatype.\\n')       # fasttext_embeddings represented as dict {key = token string : value = np.array 300 floats} (1 list = 1D)\n",
        "print(f'Each entry represents the word and the related embedding.\\n')\n",
        "\n",
        "print(f'We can query the word as a key and obtain the embedding, e.g. for good the embedding is: \\n')\n",
        "print(f'{en_embeddings[\"good\"]}.\\n')\n",
        "\n",
        "print(f'The dimensionality of these embeddings corresponds to {len(en_embeddings[\"good\"])}.')"
      ],
      "metadata": {
        "id": "Xb5dngocEwxW",
        "outputId": "4e13d12e-86f3-410f-9275-184dcba57f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loaded embeddings represent a <class 'dict'> datatype.\n",
            "\n",
            "Each entry represents the word and the related embedding.\n",
            "\n",
            "We can query the word as a key and obtain the embedding, e.g. for good the embedding is: \n",
            "\n",
            "[-0.08404064 -0.05785208  0.00155124  0.1233691  -0.05985956  0.00565746\n",
            "  0.11506542 -0.01505614  0.01587738 -0.00118624 -0.0886031   0.02126109\n",
            "  0.00912493  0.00419747  0.01450865  0.0062962   0.07829193 -0.01815862\n",
            " -0.0549321  -0.02126109  0.01076742  0.07500695  0.01359615  0.00821244\n",
            "  0.00638745 -0.05867332  0.03056853 -0.01916236  0.0617758   0.0275573\n",
            "  0.06569952 -0.05192087 -0.03987596  0.00583996  0.04005846  0.05520585\n",
            " -0.00556621 -0.11187168 -0.03221102 -0.02463732 -0.01879736  0.0068437\n",
            " -0.0062962   0.03312351 -0.03020353  0.05292461  0.00757369 -0.05785208\n",
            " -0.05274212  0.00994618 -0.08440564  0.0142349  -0.03722973  0.00611371\n",
            " -0.05812582  0.05365461  0.06579077 -0.04918339 -0.13377152 -0.03695598\n",
            " -0.02290358 -0.04516842 -0.04763215 -0.06250579  0.04261344  0.00419747\n",
            " -0.0686195   0.03312351 -0.06369203  0.01596863 -0.03129852 -0.03941971\n",
            "  0.00693495  0.05192087  0.00583996 -0.00985493  0.02126109  0.06341828\n",
            "  0.04900089  0.02308608  0.03932846 -0.03011228 -0.01158867  0.02627981\n",
            "  0.01049367 -0.02764855  0.02564106  0.08449689  0.08130316  0.2001098\n",
            "  0.03503975  0.04580717  0.12519409 -0.00784744 -0.01040242  0.09708929\n",
            " -0.07199573  0.07327322 -0.00401497 -0.04443843  0.00164249  0.06095456\n",
            "  0.00912493  0.02746605 -0.02728355  0.02290358 -0.03951096 -0.00547496\n",
            " -0.00301123 -0.0477234  -0.02992978  0.10959045  0.01067617 -0.03148102\n",
            "  0.01742862 -0.01149742 -0.01852362  0.01706363  0.04580717  0.05109963\n",
            "  0.04535092  0.03284976 -0.0062962   0.05228587  0.05429336  0.02381608\n",
            "  0.01578614 -0.0064787  -0.11816789  0.01459989  0.00136874 -0.03312351\n",
            " -0.00428872  0.02025735 -0.00638745 -0.02637106  0.03567849  0.03659099\n",
            " -0.03595224 -0.02418107 -0.04306969  0.02436357  0.06834576  0.02865229\n",
            " -0.06414828 -0.03868972 -0.18742613 -0.0828544  -0.01021993  0.05036963\n",
            " -0.10876921  0.0072087  -0.1226391  -0.01359615 -0.01907111  0.08449689\n",
            "  0.05739583 -0.02007485 -0.03339726  0.02363358  0.0408797  -0.01177116\n",
            " -0.03193727 -0.02244734 -0.04334343  0.00118624  0.01049367 -0.00483621\n",
            "  0.10183425  0.02317733 -0.03321476 -0.05411086 -0.00501871  0.0068437\n",
            " -0.01177116 -0.06560828 -0.02272109 -0.04334343  0.00173374 -0.10411549\n",
            "  0.01505614  0.04635466 -0.02445482 -0.04352593  0.00474497  0.03741223\n",
            "  0.02472857 -0.05940332 -0.06068081 -0.07281698  0.04489467  0.04909214\n",
            "  0.01834112 -0.03467475 -0.02290358  0.00374122 -0.04115345 -0.04297844\n",
            " -0.00510996  0.11926289 -0.08714312  0.15156515  0.02025735 -0.05374586\n",
            " -0.01943611  0.00392372 -0.05055213 -0.05739583 -0.04982214  0.10594048\n",
            "  0.11351417 -0.00675245  0.01605988 -0.00693495 -0.05128213 -0.01532989\n",
            "  0.07281698 -0.03695598  0.06579077  0.12154412  0.06487828 -0.0131399\n",
            "  0.02335983  0.05192087 -0.00830369 -0.00574871  0.03330601  0.0062962\n",
            "  0.0406972   0.01678988  0.00200749 -0.01907111  0.07208697 -0.05119088\n",
            "  0.02317733 -0.08823811 -0.03357976  0.01487364  0.00830369 -0.05602709\n",
            "  0.04982214 -0.03513099  0.0275573  -0.12930031  0.15631011 -0.04963964\n",
            " -0.00985493  0.04279594 -0.02637106 -0.02491107  0.08887686 -0.12245662\n",
            " -0.01843237 -0.04325218 -0.02956478  0.00492746 -0.03960221 -0.00027375\n",
            "  0.15348138  0.02837854 -0.0279223  -0.00045625  0.04279594  0.102838\n",
            " -0.12519409  0.05237712  0.00729995 -0.1581351   0.02783105 -0.11570416\n",
            " -0.05538835 -0.03503975 -0.01587738  0.11223669 -0.04854465 -0.06944074\n",
            "  0.01943611  0.01113242 -0.01177116  0.07017074 -0.24427445  0.04854465\n",
            " -0.11506542 -0.02564106  0.07281698 -0.02664481 -0.01615113 -0.09444306\n",
            " -0.04297844 -0.02764855  0.07792693  0.02673606 -0.10210801 -0.00127749\n",
            "  0.0345835   0.02226484 -0.01277491  0.12920906 -0.04671966 -0.08084691].\n",
            "\n",
            "The dimensionality of these embeddings corresponds to 300.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Loading the Bilingual Word List\n",
        "\n",
        "To perform this alignment, we will use a bilingual word list that is provided by the Multilingual Unsupervised and Supervised Embeddings (MUSE) project (see [here](https://github.com/facebookresearch/MUSE/tree/main) for all languages).\n",
        "\n",
        "ðŸ‘‹ âš’ Please change the following downloading command to the language pair of your choice (as long as available on MUSE).\n"
      ],
      "metadata": {
        "id": "0TBkmAvzAbyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-de.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "er7DPSxbgP4c",
        "outputId": "ea56976f-180d-4755-ac40-e5393542fc2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-04 16:16:35--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-de.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.51, 3.163.189.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1742131 (1.7M) [text/x-c++]\n",
            "Saving to: â€˜en-de.txtâ€™\n",
            "\n",
            "en-de.txt           100%[===================>]   1.66M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-01-04 16:16:35 (36.7 MB/s) - â€˜en-de.txtâ€™ saved [1742131/1742131]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a bilingual word list\n",
        "\n",
        "As a next step, we will create a bilingual word list from the downloaded text file.\n",
        "\n",
        "ðŸ‘‹ âš’ Create a list of tuples `[(en_word1, de_word1), (en_word2, de_word2),...]`from the downloaded text file in the following code cell. To complete this task, please complement the provided function `load_bilingual_word_list` where it says `Your code here`.\n",
        "\n",
        "For English-German, the first ten tuples of the list look like this:\n",
        "\n",
        "```\n",
        "[('the', 'die'), ('the', 'der'), ('the', 'dem'), ('the', 'den'), ('the', 'das'), ('and', 'sowie'), ('and', 'und'), ('was', 'war'), ('was', 'wurde'), ('for', 'fÃ¼r')]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UT-WD8SmAvkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bilingual_word_list(file_path: str) -> list[tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Create a list of tuples that contain word translations.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the text file with one bilingual word pair per line.\n",
        "\n",
        "    Returns:\n",
        "        list of tuple: A list of tuples, each containing one bilingual word pair (en_word, de_word).\n",
        "    \"\"\"\n",
        "    bilingual_dict = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Skip empty lines\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            # Split the line by whitespace\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                en_word, de_word = parts\n",
        "                bilingual_dict.append((en_word, de_word))\n",
        "    return bilingual_dict\n",
        "\n",
        "\n",
        "# Load English-German word pairs\n",
        "en_de_pairs = load_bilingual_word_list('en-de.txt')\n",
        "\n",
        "print(en_de_pairs[:10])"
      ],
      "metadata": {
        "id": "snp8ndm5gZEa",
        "outputId": "1c8345c3-f99c-4470-e5ba-4c961472aa06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'die'), ('the', 'der'), ('the', 'dem'), ('the', 'den'), ('the', 'das'), ('and', 'sowie'), ('and', 'und'), ('was', 'war'), ('was', 'wurde'), ('for', 'fÃ¼r')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the Embeddings for our Word List\n",
        "\n",
        "As a next step, we need to see which words from the word list have a vector representation in the embedding space for both languages and create a list of corresponding embeddings for both languages.\n"
      ],
      "metadata": {
        "id": "wCujQTNRLHFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def extract_word_embeddings(\n",
        "    bilingual_pairs: list[tuple[str, str]],\n",
        "    en_embeddings: dict[str, np.ndarray],\n",
        "    de_embeddings: dict[str, np.ndarray]\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Function to create a list of word embeddings that is parallel to a bilingual list of words.\n",
        "\n",
        "    Parameters:\n",
        "        Bilingual list of words, embeddings in the first language, embeddings in the second language.\n",
        "\n",
        "    Returns:\n",
        "        Two numpy arrays of embeddings that correspond to the bilingual word list.\n",
        "    \"\"\"\n",
        "    en_vecs = []\n",
        "    de_vecs = []\n",
        "\n",
        "    for en_word, de_word in bilingual_pairs:\n",
        "        if en_word in en_embeddings and de_word in de_embeddings:\n",
        "            en_vecs.append(en_embeddings[en_word])\n",
        "            de_vecs.append(de_embeddings[de_word])\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    en_vecs = np.array(en_vecs)\n",
        "    de_vecs = np.array(de_vecs)\n",
        "\n",
        "    return en_vecs, de_vecs\n",
        "\n",
        "# Extract English and German embeddings for the bilingual lexicon\n",
        "en_vecs, de_vecs = extract_word_embeddings(en_de_pairs, en_embeddings, de_embeddings)\n",
        "\n",
        "print(f\"Extracted {en_vecs.shape[0]} aligned word vectors.\")"
      ],
      "metadata": {
        "id": "ei3DaO3mgnX5",
        "outputId": "5c424577-cc20-4ed5-f19d-003b75680967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 22546 aligned word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Embedding Alignment**\n",
        "\n",
        "We will now use the dictionary and embeddings to align the two vector spaces. The English vector space will be aligned to the German vector space using the [Procrustes](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem) alignment method.\n",
        "\n",
        "Given two matrices, Procrustes finds an orthogonal matrix which most closely maps one input matrix to the other. As a first step, we need to compute this orthogonal transformation matrix.  \n",
        "\n"
      ],
      "metadata": {
        "id": "fZ8nhE-4LXNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def orthogonal_procrustes(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Function to perform orthogonal Procrustes alignment to learn a mapping from X to Y.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy array): Source language word embeddings (English).\n",
        "        Y (numpy array): Target language word embeddings (German).\n",
        "\n",
        "    Returns:\n",
        "        W (numpy array): Orthogonal transformation matrix.\n",
        "    \"\"\"\n",
        "    X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute matrix product of X^T and Y\n",
        "    M = np.dot(X.T, Y)\n",
        "\n",
        "    # Perform Singular Value Decomposition (SVD) on the matrix M\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "\n",
        "    # Compute the orthogonal transformation matrix W\n",
        "    W = np.dot(U, Vt)\n",
        "\n",
        "    return W\n",
        "\n",
        "W = orthogonal_procrustes(en_vecs, de_vecs)\n",
        "\n",
        "print(\"Orthogonal mapping matrix learned.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmA3RxaSg3-C",
        "outputId": "cf6b8a9f-3e50-4e47-af4a-c337839b4772"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orthogonal mapping matrix learned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a second step, the obtained matrix is used to learn an orthogonal mapping of the English vector space to approximate it to the German vector space. Here we can transform the entire vector space of 100,000 embeddings."
      ],
      "metadata": {
        "id": "YINE8wlfIbk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mapping(embeddings: dict[str, np.ndarray], W: np.ndarray) -> dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Apply the learned orthogonal mapping to the source language embeddings.\n",
        "\n",
        "    Parameters:\n",
        "        embeddings (dict): Source language embeddings (English)\n",
        "        W (numpy array): Orthogonal transformation matrix\n",
        "\n",
        "    Returns:\n",
        "        mapped_embeddings (dict): Transformed embeddings\n",
        "    \"\"\"\n",
        "    mapped_embeddings = {}\n",
        "    for word, vec in embeddings.items():\n",
        "        mapped_vec = np.dot(vec, W)\n",
        "        # Normalize the mapped vector\n",
        "        mapped_vec = mapped_vec / np.linalg.norm(mapped_vec)\n",
        "        mapped_embeddings[word] = mapped_vec\n",
        "    return mapped_embeddings\n",
        "\n",
        "aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "\n",
        "print(f\"Aligned {len(aligned_en_embeddings)} English embeddings into the German space.\")        # NB: Why en -> de? because german embeddings need to remain unchanged? -> better comparison accross lang.?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjogY41Vg-CC",
        "outputId": "5b364b82-3535-4b42-c4bc-932bbec752ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned 100000 English embeddings into the German space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Evaluation**\n",
        "\n",
        "In this part, you will explore two different tasks for evaluating the final vector space:\n",
        "\n",
        "\n",
        "1.   Word Translation\n",
        "2.   Cross-Lingual Analogy Completion"
      ],
      "metadata": {
        "id": "Lr2QCywEBMUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Translation\n",
        "\n",
        "We will now use the bilingual word list downloaded from MUSE to evaluate the ability of our newly created aligned embedding space to translate words from English to German.\n",
        "\n",
        "A function that takes an English word as input and ouputs the nearest neighors (KNN) of the German vector space is already provided for your convenience."
      ],
      "metadata": {
        "id": "nTo8Ugi-KGAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def get_nn(word, aligned_en_embeddings, de_embeddings, top_k):\n",
        "    en_vec = aligned_en_embeddings[word]\n",
        "    de_words = list(de_embeddings.keys())\n",
        "    de_vecs = np.array(list(de_embeddings.values()))\n",
        "\n",
        "    # Compute cosine similarity between the English word vector and all German word vectors (+normalize vectors)\n",
        "    en_vec = en_vec / np.linalg.norm(en_vec)\n",
        "    de_vecs_norm = de_vecs / np.linalg.norm(de_vecs, axis=1, keepdims=True)\n",
        "    similarities = cosine_similarity([en_vec], de_vecs_norm).flatten()\n",
        "\n",
        "    # Get top_k most similar German words\n",
        "    nearest_idxs = similarities.argsort()[-top_k:][::-1]\n",
        "    nearest_words = [de_words[i] for i in nearest_idxs]\n",
        "\n",
        "    return nearest_words\n",
        "\n",
        "en_word = 'the'\n",
        "nearest_neighbors = get_nn(en_word, aligned_en_embeddings, de_embeddings, 5)\n",
        "print(f\"Nearest neighbors of '{en_word}': {nearest_neighbors}\")"
      ],
      "metadata": {
        "id": "p63UvUddLKWG",
        "outputId": "681ffaf6-a0c7-4f78-b9bb-8ca4b811b548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors of 'the': ['der', 'die', 'den', 'dem', 'besagten']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Use the already downloaded bilingual word list to evaluate the ability of our aligned vector space to translate from English to German. The output of this task should be the **accuracy** calculated on **1000 words** from the word list, i.e., how many of the first 1000 English words result in five German neighbors that correspond to the German translation from the MUSE word list.\n",
        "\n",
        "Use the provided function `get_nn` to obtain the *k* nearest words in the vector space in German, given an English input word.\n"
      ],
      "metadata": {
        "id": "zagwMc7uNqmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n",
        "def evaluate_translation_accuracy(bilingual_pairs, aligned_en_embeddings, de_embeddings, top_k=5, num_words=1000, print_limit=20):\n",
        "    \"\"\"\n",
        "    Evaluate the accuracy of the aligned vector space for translation.\n",
        "\n",
        "    Parameters:\n",
        "        bilingual_pairs (list of tuple): List of bilingual word pairs (English, German).\n",
        "        aligned_en_embeddings (dict): Aligned English word embeddings.\n",
        "        de_embeddings (dict): German word embeddings.\n",
        "        top_k (int): Number of nearest neighbors to consider for translation.\n",
        "        num_words (int): Number of English words to evaluate.\n",
        "        print_limit (int): Maximum number of words to print nearest neighbors for.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of translations.\n",
        "    \"\"\"\n",
        "    correct_translations = 0        # Increases when correct german translation (taken from the bilingual MUSE word list) is found among k=5 nn of english embedding\n",
        "    total_translations = 0          # Tracks total number of bilingual MUSE pairs evaluated (both eng and ger words taken from MUSE have valid embeddings in embedding space)\n",
        "    printed = 0                     # Counter for printed nearest neighbors\n",
        "    skipped_pairs = 0               # Counter for skipped pairs due to missing embeddings\n",
        "\n",
        "    # Iterate over the first \"num_words\" bilingual pairs; here: 1000\n",
        "    for en_word, de_word in bilingual_pairs[:num_words]:\n",
        "        if en_word in aligned_en_embeddings and de_word in de_embeddings:\n",
        "            # Get the top-k nearest German neighbors\n",
        "            nearest_neighbors = get_nn(en_word, aligned_en_embeddings, de_embeddings, top_k)\n",
        "\n",
        "            # Print only the first `print_limit` English words and their neighbors\n",
        "            if printed < print_limit:\n",
        "                print(f\"English word: '{en_word}' -> Nearest German neighbors: {nearest_neighbors}\")\n",
        "                printed += 1\n",
        "\n",
        "            # Check if the correct German word is among the top-k nearest neighbors\n",
        "            if de_word in nearest_neighbors:\n",
        "                correct_translations += 1\n",
        "            total_translations += 1\n",
        "        else:\n",
        "            skipped_pairs += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_translations / total_translations if total_translations > 0 else 0\n",
        "\n",
        "    return accuracy, correct_translations, total_translations, skipped_pairs\n",
        "\n",
        "# Evaluate translation accuracy using the first 1000 words from the bilingual word list\n",
        "accuracy, correct_translations, total_translations, skipped_pairs = evaluate_translation_accuracy(\n",
        "    bilingual_pairs=en_de_pairs,\n",
        "    aligned_en_embeddings=aligned_en_embeddings,\n",
        "    de_embeddings=de_embeddings,\n",
        "    top_k=5,\n",
        "    num_words=1000,\n",
        "    print_limit=20      # Limited to print only the first 20 words\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}% ({correct_translations}/{total_translations})\")\n",
        "print(f\"Skipped pairs: {skipped_pairs}\")"
      ],
      "metadata": {
        "id": "6pb9wd7HOUR5",
        "outputId": "a9186504-2477-4e00-d2c2-a338291ba24c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English word: 'the' -> Nearest German neighbors: ['der', 'die', 'den', 'dem', 'besagten']\n",
            "English word: 'the' -> Nearest German neighbors: ['der', 'die', 'den', 'dem', 'besagten']\n",
            "English word: 'the' -> Nearest German neighbors: ['der', 'die', 'den', 'dem', 'besagten']\n",
            "English word: 'the' -> Nearest German neighbors: ['der', 'die', 'den', 'dem', 'besagten']\n",
            "English word: 'the' -> Nearest German neighbors: ['der', 'die', 'den', 'dem', 'besagten']\n",
            "English word: 'and' -> Nearest German neighbors: ['und', 'sowie', 'udn', 'aber', 'sodass']\n",
            "English word: 'and' -> Nearest German neighbors: ['und', 'sowie', 'udn', 'aber', 'sodass']\n",
            "English word: 'was' -> Nearest German neighbors: ['war', 'wurde', 'hatte', 'damals', 'waren']\n",
            "English word: 'was' -> Nearest German neighbors: ['war', 'wurde', 'hatte', 'damals', 'waren']\n",
            "English word: 'for' -> Nearest German neighbors: ['fÃ¼r', 'FÃ¼r', 'vor', 'dafÃ¼r', 'bei']\n",
            "English word: 'that' -> Nearest German neighbors: ['dass', 'tatsÃ¤chlich', 'glaube', 'aber', 'denn']\n",
            "English word: 'that' -> Nearest German neighbors: ['dass', 'tatsÃ¤chlich', 'glaube', 'aber', 'denn']\n",
            "English word: 'with' -> Nearest German neighbors: ['mit', 'zusammen', 'Zusammen', 'Mit', 'neben']\n",
            "English word: 'from' -> Nearest German neighbors: ['aus', 'von', 'stammen', 'vom', 'weg']\n",
            "English word: 'from' -> Nearest German neighbors: ['aus', 'von', 'stammen', 'vom', 'weg']\n",
            "English word: 'from' -> Nearest German neighbors: ['aus', 'von', 'stammen', 'vom', 'weg']\n",
            "English word: 'from' -> Nearest German neighbors: ['aus', 'von', 'stammen', 'vom', 'weg']\n",
            "English word: 'this' -> Nearest German neighbors: ['diesem', 'dieses', 'dieser', 'diese', 'das']\n",
            "English word: 'this' -> Nearest German neighbors: ['diesem', 'dieses', 'dieser', 'diese', 'das']\n",
            "English word: 'this' -> Nearest German neighbors: ['diesem', 'dieses', 'dieser', 'diese', 'das']\n",
            "Accuracy: 54.76% (437/798)\n",
            "Skipped pairs: 202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the output:\n",
        "\n",
        "1. The same English word might appear multiple times because of duplicate entries in the bilingual dictionary of \"MUSE\" (e.g.: ('player', 'spieler'), ('player', 'Mitspieler'), ('player', 'Spieler')).\n",
        "\n",
        "2. In the aligned embedding space the nearest neighbors are always the same because the embedding for the English word and the German embedding space are static and consistent across evaluations.\n"
      ],
      "metadata": {
        "id": "wdVdrsRC0nvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Lingual Analogy Completion\n",
        "\n",
        "An analogy compares two related pairs of words, e.g. *man is to woman as king is to queen*. This task can be extended to use analogies for translation, e.g. *man is to woman as Mann ist zu Frau*.\n",
        "\n",
        "\n",
        "ðŸ‘‹ âš’ Create **twenty** examples of crosslingual analogies and see whether the aligned vector space is able to correctly complete analogies across languages, e.g. positive=(queen, KÃ¶nig), negative=(king). You can use examples from the analogy text file in GitHub for this purpose.\n",
        "\n",
        "Hints:\n",
        "\n",
        "\n",
        "*   Multilingual Analogies: To create the examples, all you need is a translation of an existing analogy. You can use the already loaded bilingual word list to obtain the translations and the existing analogy list (anlogies.txt on Github) to obtain analogies.\n",
        "*   Implementation: In the code below, you only need to change the embeddings to the German embeddings for `c` and provide the function with the German embeddings."
      ],
      "metadata": {
        "id": "Xj24LdO-Omrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def norm(vec):\n",
        "    return vec / np.linalg.norm(vec)        # NB: normalization: length=1\n",
        "\n",
        "def get_target_words(embeddings, vec_a, vec_b, vec_c, top_k):\n",
        "    words = list(embeddings.keys())\n",
        "    vecs = np.array(list(embeddings.values()))\n",
        "\n",
        "    # Compute analogy based on input vectors b+c-a (woman+king-man)\n",
        "    positive = norm(vec_b+vec_c)\n",
        "    target_vec = norm(positive - vec_a)\n",
        "    vecs_norm = vecs / np.linalg.norm(vecs, axis=1, keepdims=True)\n",
        "    similarities = cosine_similarity([target_vec], vecs_norm).flatten()\n",
        "\n",
        "    # Get top_k most similar words for the retrieved result vector d\n",
        "    nearest_idxs = similarities.argsort()[-top_k:][::-1]\n",
        "    nearest_words = [words[i] for i in nearest_idxs]\n",
        "\n",
        "    return nearest_words\n",
        "\n",
        "# Cross-lingual examples\n",
        "cross_lingual_examples = [\n",
        "    # Format: (English word1, English word2, German word1, Expected German word2)\n",
        "    (\"Athens\", \"Greece\", \"Berlin\", \"Deutschland\"),\n",
        "    (\"Ottawa\", \"Canada\", \"Paris\", \"Frankreich\"),\n",
        "    (\"Madrid\", \"Spain\", \"Rom\", \"Italien\"),\n",
        "    (\"London\", \"England\", \"Moskau\", \"Russland\"),\n",
        "    (\"Helsinki\", \"Finland\", \"Stockholm\", \"Schweden\"),\n",
        "    (\"man\", \"woman\", \"Mann\", \"Frau\"),\n",
        "    (\"king\", \"queen\", \"KÃ¶nig\", \"KÃ¶nigin\"),\n",
        "    (\"brother\", \"sister\", \"Bruder\", \"Schwester\"),\n",
        "    (\"teacher\", \"student\", \"Lehrer\", \"SchÃ¼ler\"),\n",
        "    (\"cat\", \"dog\", \"Katze\", \"Hund\"),\n",
        "    (\"car\", \"bicycle\", \"Auto\", \"Fahrrad\"),\n",
        "    (\"book\", \"pen\", \"Buch\", \"Stift\"),\n",
        "    (\"doctor\", \"nurse\", \"Arzt\", \"Krankenschwester\"),\n",
        "    (\"city\", \"village\", \"Stadt\", \"Dorf\"),\n",
        "    (\"apple\", \"orange\", \"Apfel\", \"Orange\"),\n",
        "    (\"morning\", \"night\", \"Morgen\", \"Nacht\"),\n",
        "    (\"summer\", \"winter\", \"Sommer\", \"Winter\"),\n",
        "    (\"ocean\", \"river\", \"Ozean\", \"Fluss\"),\n",
        "    (\"mountain\", \"valley\", \"Berg\", \"Tal\"),\n",
        "    (\"sun\", \"moon\", \"Sonne\", \"Mond\")\n",
        "]\n",
        "\n",
        "# Validate each analogy\n",
        "for en_word1, en_word2, de_word1, expected_de_word2 in cross_lingual_examples:\n",
        "    if en_word1 in aligned_en_embeddings and en_word2 in aligned_en_embeddings and de_word1 in de_embeddings:\n",
        "        vec_a = norm(aligned_en_embeddings[en_word1])       # English word1\n",
        "        vec_b = norm(aligned_en_embeddings[en_word2])       # English word2\n",
        "        vec_c = norm(de_embeddings[de_word1])       # German word1\n",
        "        nearest_neighbors = get_target_words(de_embeddings, vec_a, vec_b, vec_c, top_k=1)\n",
        "\n",
        "        print(f\"Analogy: {en_word1} : {en_word2} :: {de_word1} : ?\")\n",
        "        print(f\"Expected: {expected_de_word2}, Predicted: {nearest_neighbors[0]}\\n\")\n",
        "    else:\n",
        "        print(f\"Skipping analogy: {en_word1}, {en_word2}, {de_word1} (missing embeddings)\\n\")"
      ],
      "metadata": {
        "id": "S40JvI6iOzMA",
        "outputId": "982e6869-bf67-480c-afb7-fcf6abe57f81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy: Athens : Greece :: Berlin : ?\n",
            "Expected: Deutschland, Predicted: Berlin\n",
            "\n",
            "Analogy: Ottawa : Canada :: Paris : ?\n",
            "Expected: Frankreich, Predicted: Paris\n",
            "\n",
            "Analogy: Madrid : Spain :: Rom : ?\n",
            "Expected: Italien, Predicted: Rom\n",
            "\n",
            "Analogy: London : England :: Moskau : ?\n",
            "Expected: Russland, Predicted: Russland\n",
            "\n",
            "Analogy: Helsinki : Finland :: Stockholm : ?\n",
            "Expected: Schweden, Predicted: Schweden\n",
            "\n",
            "Analogy: man : woman :: Mann : ?\n",
            "Expected: Frau, Predicted: Freundinnen\n",
            "\n",
            "Analogy: king : queen :: KÃ¶nig : ?\n",
            "Expected: KÃ¶nigin, Predicted: KÃ¶nigin\n",
            "\n",
            "Analogy: brother : sister :: Bruder : ?\n",
            "Expected: Schwester, Predicted: Schwester\n",
            "\n",
            "Analogy: teacher : student :: Lehrer : ?\n",
            "Expected: SchÃ¼ler, Predicted: Studenten\n",
            "\n",
            "Analogy: cat : dog :: Katze : ?\n",
            "Expected: Hund, Predicted: HÃ¼ndin\n",
            "\n",
            "Analogy: car : bicycle :: Auto : ?\n",
            "Expected: Fahrrad, Predicted: Fahrrad\n",
            "\n",
            "Analogy: book : pen :: Buch : ?\n",
            "Expected: Stift, Predicted: Stift\n",
            "\n",
            "Analogy: doctor : nurse :: Arzt : ?\n",
            "Expected: Krankenschwester, Predicted: Hotelier\n",
            "\n",
            "Analogy: city : village :: Stadt : ?\n",
            "Expected: Dorf, Predicted: Dorfgemeinschaft\n",
            "\n",
            "Analogy: apple : orange :: Apfel : ?\n",
            "Expected: Orange, Predicted: Mittelstreifen\n",
            "\n",
            "Analogy: morning : night :: Morgen : ?\n",
            "Expected: Nacht, Predicted: Nacht\n",
            "\n",
            "Analogy: summer : winter :: Sommer : ?\n",
            "Expected: Winter, Predicted: Winter\n",
            "\n",
            "Analogy: ocean : river :: Ozean : ?\n",
            "Expected: Fluss, Predicted: Fluss\n",
            "\n",
            "Analogy: mountain : valley :: Berg : ?\n",
            "Expected: Tal, Predicted: Esch\n",
            "\n",
            "Analogy: sun : moon :: Sonne : ?\n",
            "Expected: Mond, Predicted: Mond\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
